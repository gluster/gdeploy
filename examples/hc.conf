# A sample configuration file to setup ROBO

[hosts]
host1
host2
host3

[disktype]
raid6

[diskcount]
4

[stripesize]
256

[RH-subscription1]
action=register
username=qa@redhat.com
password=<passwd>
pool=<pool>

[RH-subscription2]
action=disable-repos
repos=

[RH-subscription3]
action=enable-repos
repos=rhel-7-server-rpms,rh-gluster-3-for-rhel-7-server-rpms,rhel-7-server-rhv-4-mgmt-agent-rpms

[yum1]
action=install
gpgcheck=yes
packages=vdsm,vdsm-gluster,ovirt-hosted-engine-setup,screen,gluster-nagios-addons
update=yes

# Setup ntp on the servers before any other operations are done
# Disable the existing public servers
[shell1]
action=execute
command=sed -i 's/^\(server .*iburst\)/#\1/' /etc/ntp.conf

# Add custom NTP server
[update-file1]
action=add
dest=/etc/ntp.conf
line=server clock.redhat.com iburst

[service1]
action=enable
service=ntpd

[service2]
action=restart
service=ntpd

[shell2]
action=execute
command=vdsm-tool configure --force

[service3]
action=start
service=vdsmd

# Disable multipath
[script1]
action=execute
file=/usr/share/ansible/gdeploy/scripts/disable-multipath.sh

[pv]
action=create
devices=vdb

[vg1]
action=create
vgname=RHGS_vg1
pvname=vdb

[lv1]
action=create
vgname=RHGS_vg1
lvname=engine_lv
lvtype=thick
size=10GB
mount=/rhgs/brick1

[lv2]
action=create
vgname=RHGS_vg1
poolname=lvthinpool
lvtype=thinpool
poolmetadatasize=10MB
chunksize=1024k
size=30GB

[lv3]
action=create
lvname=lv_vmaddldisks
poolname=lvthinpool
vgname=RHGS_vg1
lvtype=thinlv
mount=/rhgs/brick2
virtualsize=9GB

[lv4]
action=create
lvname=lv_vmrootdisks
poolname=lvthinpool
vgname=RHGS_vg1
size=19GB
lvtype=thinlv
mount=/rhgs/brick3
virtualsize=19GB

[selinux]
yes

[vg2]
action=extend
vgname=RHGS_vg1
pvname=vdc

[lv5]
action=setup-cache
ssd=sdc
vgname=RHGS_vg1
poolname=lvthinpool
cache_lv=lvcache
cache_lvsize=5GB

[service4]
action=stop
service=NetworkManager

[service5]
action=disable
service=NetworkManager

[shell3]
action=execute
command=mkdir /etc/systemd/system/glusterd.service.d

[shell4]
action=execute
command=echo -e "[Service]\nCPUAccounting=yes\nSlice=glusterfs.slice" >> /etc/systemd/system/glusterd.service.d/99-cpu.conf,

[shell5]
action=execute
command=echo -e "[Slice]\nCPUQuota=400%" >> /etc/systemd/system/glusterfs.slice,systemctl daemon-reload

[service6]
action=restart
service=glusterd

[firewalld]
action=add
ports=111/tcp,2049/tcp,54321/tcp,5900/tcp,5900-6923/tcp,5666/tcp,16514/tcp
services=glusterfs

[update-file2]
action=edit
dest=/etc/nagios/nrpe.cfg
replace=allowed_hosts
line=allowed_hosts=host.redhat.com

[service7]
action=restart
service=nrpe

[script2]
action=execute
file=/usr/share/ansible/gdeploy/scripts/disable-gluster-hooks.sh

[volume1]
action=create
volname=engine
transport=tcp
replica=yes
replica_count=3
key=group,storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,locking-scheme,shd-max-threads,cluster.shd-wait-qlength,cluster.shd-max-threads,network.ping-timeout,user.cifs,nfs.disable
value=virt,36,36,on,512MB,32,full,granular,8,10000,6,30,off,enable
brick_dirs=/rhgs/brick1/engine

[volume2]
action=create
volname=vmstore
transport=tcp
replica=yes
replica_count=3
key=group,storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,locking-scheme,shd-max-threads,cluster.shd-wait-qlength,cluster.shd-max-threads,network.ping-timeout,user.cifs,nfs.disable
value=virt,36,36,on,512MB,32,full,granular,8,10000,6,30,off,enable
brick_dirs=/rhgs/brick2/vmstore

[volume3]
action=create
volname=data
transport=tcp
replica=yes
replica_count=3
key=group,storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm,locking-scheme,shd-max-threads,cluster.shd-wait-qlength,cluster.shd-max-threads,network.ping-timeout,user.cifs,nfs.disable
value=virt,36,36,on,512MB,32,full,granular,8,10000,6,30,off,enable
brick_dirs=/rhgs/brick3/data

[yum2:host1]
action=install
gpgcheck=no
packages=rhevm-appliance

# [shell6]
# action=execute
# command=reboot

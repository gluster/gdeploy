[hosts]
10.70.46.13
10.70.46.15
10.70.46.17
10.70.46.19
#------------------------------

# NOTE: Patterns are supported for all the multiple values options and sections.
# For example: hosts can be given as 10.70.46.1{3..7} specifying the 4 hosts
# ranging from 10.70.46.13 to 10.70.46.27



# Generic section [devices] is applicable to all the hosts listed in the [hosts]
# section. However, if sections of hosts [hostname] or [ip-address] is present,
# then the data in generic sections like [devices] are ignored. Host specific
# data take precedence.

# [devices]
# /dev/sda
# /dev/vdb
# /dev/vda
#------------------------------

# Section [disktype] specifies which disk configuration is used while
# setting up the back-end. Supports RAID 10, RAID 6 and JBOD configurations.
# If this field is left empty, it will be by default taken as JBOD.
# This is common for all the hosts.

# [disktype]
# raid6
#------------------------------

# Section [diskcount] specifies the number of data disks in the setup. This is a
# mandatory field if the disk configuration specified is either RAID 10 or
# RAID 6 and will be ignored if architecture is JBOD. This is host specific
# data.

# [diskcount]
# 10
#------------------------------

# Section [stripesize] specifies the stripe_unit size in KB. This is a mandatory
# field if disk configuration is RAID 6. If this is not specified in case of
# RAID 10 configurations, it will take the default value 256K. This field is
# not necessary for JBOD configuration of disks. Do not add any suffixes like
# K, KB, M, etc.This is host specific data.

# [stripesize]
# 128
#------------------------------

# vg names for the above devices
# The number of vgs in the [vgs] should match the devices

# [vgs]
# CUSTOM_vg
#------------------------------

# pool names for the above volume groups
# The number of pools listed in the [pools] section should match the number of
# vgs.

# [pools]
# CUSTOM_pool
#------------------------------

# lv names for the above volume groups
# The number of logical volumes listed in the [lvs] section should match the
# number of vgs.

# [lvs]
# CUSTOM_lv
#-------------------------------

# Brick mountpoints for the logical volumes
# The number of mountpoints should match the number of logical volumes listed
# above.
# IMP: If only gluster deployment is to be done and not back-end setup, just
# give this data about the hosts specified in the [hosts] section along
# with the client data.

# [mountpoints]
# /gluster/brick
#-------------------------------

# brick_dirs is the directory which is to be used as brick while creating the
# volume. A mountpoint cannot be used as a brick directory, so brick_dirs
# specifies the directory to be made inside the LV mount point that will be
# used as a brick.
# This field can be left empty in which case a directory will be created
# inside the mountpoint with a default name. If backend setup is not being done
# this field will be ignored.

# [brick_dirs]
# gluster_brick


# Host specific data are to be given as follows

[10.70.46.13]
devices=/dev/sdb,/dev/vdb,/dev/vda
vgs=CUSTOM_vg1,CUSTOM_vg2,CUSTOM_vg3
pools=CUSTOM_pool1,CUSTOM_pool2,CUSTOM_pool3
lvs=CUSTOM_lv1,CUSTOM_lv2,CUSTOM_lv3
mountpoints=/gluster/brick1,/gluster/brick2,/gluster/brick3
brick_dirs=glusterbrick{1..3}

# NOTE: The host specific data of 10.70.46.15 and 10.70.46.17 are
# the same. It can be fed either of the ways. Two separate
# ways defines are just to demonstrate the format of
# patterns usage

[10.70.46.15]
devices=/dev/vd{b..c}
vgs=CUSTOM_vg{1..2}
pools=CUSTOM_pool{1..2}
lvs=CUSTOM_lv{1..2}
mountpoints=/gluster/brick{1..2}

[10.70.46.17]
devices=/dev/vdc,/dev/vdb
vgs=CUSTOM_vg1,CUSTOM_vg2
pools=CUSTOM_pool1,CUSTOM_pool2
lvs=CUSTOM_lv1,CUSTOM_lv2
mountpoints=/gluster/brick1,/gluster/brick2

[10.70.46.19]
devices=/dev/vdb
vgs=CUSTOM_vg1
pools=CUSTOM_pool1
lvs=CUSTOM_lv1
mountpoints=/gluster/brick1
#----------------------------------


                            ##-- Peer --##


# The section peer specifies the configurations for the Trusted Storage
# Pool management(TSP)

# This section helps in making all the hosts specified in the section 'hosts'
# to either probe each other making the TSP or detach all of them from TSP

# The only option in this section is the option names 'manage' which can have
# it's values to be  either probe or detach

# To do a peer probe

[peer]
manage=probe

# To do a peer detach

# [peer]
# manage=detach

#-----------------------------------


                            ##-- Volume --##



# The section volume specifies the configuration options for the volume.

# 'action' option specifies what action id to be performed in the volume.
# The choices are: [create, delete, add-brick, remove-brick, rebalance].

# If delete is provided all the options other than 'volname' will be ignored.

# If add-brick or remove-brick is chosen, extra option bricks with a
# comma separated list of brick names(in the format <hostname>:<brick path>
# should be provided.

# In case of remove-brick and rebalance, 'state' option should also
# be provided. Choices for 'state' are:
# For remove-brick: [start, stop, commit, force]
# For rebalance: [start, stop, fix-layout]

# 'volname' option specifies the volume name. Default is glustervol
#  If the user wishes to do just a volume operation, she can omit the
#  'hosts' section if the volname is provided in the format
#  <hostname>:<volname>, where hostname is the hostname or IP of one of
#  the nodes in the cluster

#  IMP: Only single volume creation/deletion/configuration is supported
#  as of now.

# 'transport' option specifies the transport type. Default is tcp. Options are
# tcp or rdma or tcp,rdma

# 'replica' option will specify if the volume should be of type replica or not.
# options are yes and no. Default is no.
# If 'replica' is given as yes, 'replica_count' should be given.
# Option 'arbiter_count' is optional.

# 'disperse' option will specify if the volume should be of type disperse.
# options are yes and no. Default is no.
# 'disperse_count' is optional even if the 'disperse' is yes. if not specified,
# the number of bricks specified in the command line is taken as the
# disperse_count value.
# If 'redundancy_count' is not specified, and if 'disperse' is yes,  it's
# default value is computed so that it generates an optimal configuration.
# An option 'force' can be used, in case the brick_dirs specified are
# some mountpoints and must be used anyway.


# For creating a volume of type disperse:

[volume]
action=create
volname=glustervol
transport=tcp,rdma
disperse=yes
disperse_count=0
redundancy_count=2
force=yes



# For creating a volume of type replicate:

# [volume]
# action=create
# volname=glustervol
# transport=tcp,rdma
# replica=yes
# replica_count=2
# arbiter_count=1
# force=yes



# To start a volume

# [volume]
# action=start
# volname=10.70.46.13:glustervol



# To stop a volume

# [volume]
# action=stop
# volname=10.70.46.13:glustervol



# To delete a volume

# [volume]
# action=delete
# volname=10.70.46.13:glustervol



# Add brick operation can be done by using:

# [volume]
# action=add-brick
# volname=glustervol
# bricks=10.70.46.13:/mnt/new_brick{1..8}

# Remove brick operation can be done by using:

# [volume]
# action=remove-brick
# volname=10.70.46.13:glustervol
# bricks=10.70.46.17:/mnt/brick{1..8}
# state=commit

# Rebalance operation can be done by using:

# [volume]
# action=rebalance
# volname=10.70.46.13:glustervol
# state=start


#-----------------------------------


                            ##-- Clients --##


# IMP: If only back-end setup is to be done but not GlusterFs
# deployment omit the following section

# Specifies the client hosts and client_mount_points to mount the gluster
# volume created.
# 'action' option is to be specified for the framework to understand
# what action is to be done.
# The choices are: ['mount', 'unmount']
# 'hosts' field is mandatory.
# The option 'fstype' specifies how gluster volume is to be mounted.
# Choices are: [glusterfs, nfs] (Default is glusterfs)

# If the fstype is given as nfs, the version by default will be taken as
# 3. This can be changed by providing the option 'nfs-version'.
# eg: nfs-version=4

# Each client can have different types of volume mount. Just specify it comma
# separated.

# Option 'client_mount_points' specifies where the clients are to be mounted
# in each host. Each host can have a separate mountpoint, in which case it will
# be given comma separated or else every mountpoint can have a mountpoint of the
# same name. If 'client_mount_points'
# are not specified, default will be taken as /mnt/gluster
# for all the hosts


# Mounting a volume using native fuse on 3 client hosts

[clients]
action=mount
# volname=glustervol (If not specified earlier in 'volume' section
hosts=10.70.46.1{3..5}
fstype=glusterfs
client_mount_points=/mnt/gluster{1..3}


# Mounting a volume using nfs on a single client host

# [clients]
# action=mount
# # volname=glustervol (If not specified earlier in 'volume' section
# hosts=10.70.46.13
# fstype=nfs
# nfs-version=4
# client_mount_points=/mnt/gluster

#-----------------------------------


                            ##-- Snapshot --##


# 'snapshot' section can be used if the user wants to create or delete
# a snapshot.
# The option 'action' is to be used to specify which snapshot action is to be
# executed.
# The choices are [create, delete, clone, config, and restore]


# For snapshot creation:

# The name of the snapshot can be specified as the value to the snapname option.
# If the action is create the name of the volume is to specified as the value
# to the option 'volname'.(If not specified under the volume section).

# [snapshot]
# action=create
# snapname=glustersnap

# For snapshot deletion:

# [snapshot]
# action=delete
# snapname=glustersnap
## volname=glustervol [alternative]

# For snapshot cloning:

# [snapshot]
# action=delete
# snapname=glustersnap
# clonename=an_old_snap

# For snapshot configure:

# [snapshot]
# action=config
# snap_max_soft_limit=92
# snap_max_hard_limit=95
# auto_delete=disable
# activate_on_create=enable

# For snapshot restore:

# [snapshot]
# action=restore
# snapname=glustersnap

#-----------------------------------


                            ##-- Quota --##

# 'quota' section can be used to set quota limits on mounted
# volume directories and sub directories. The actions supported are
#     [enable, disable, remove, remove-objects, default-soft-limit,
#     limit-usage, limit-objects, alert-time, soft-timeout, hard-timeout].
#
# For enabling quota:
#
# [quota]
# action=enable
# volname=glustervol
# client_hosts=10.70.46.23,10.70.46.24
#
#
# For disabling quota:
#
# [quota]
# action=disable
# volname=glustervol
# client_hosts=10.70.46.23,10.70.46.24
#
#
# For removing quota limits on a path
#
# [quota]
# action=remove
# volname=glustervol
# client_hosts=10.70.46.23,10.70.46.24
# path=/,/dir1
#
#
# For removing quota objects on a path
#
# [quota]
# action=remove-objects
# volname=glustervol
# client_hosts=10.70.46.23,10.70.46.24
# path=/,/dir1
#
# For setting default soft limits
#
# [quota]
# action=default-soft-limit
# volname=glustervol
# client_hosts=10.70.46.23,10.70.46.24
# percent=85
#
#
# For limiting usage for volume
#
# [quota]
# action=limit-usage
# volname=glustervol
# client_hosts=10.70.46.23,10.70.46.24
# path=/,/dir1
# size=5MB,6MB
#
#
# For limiting object count for volume
#
# [quota]
# action=limit-objects
# volname=glustervol
# client_hosts=10.70.46.23,10.70.46.24
# path=/,/dir1
# number=10,20
#
#
# For setting alert-time
#
# [quota]
# action=alert-time
# volname=glustervol
# client_hosts=10.70.46.23,10.70.46.24
# time=1W
#
#
# For setting soft-timeout
#
# [quota]
# action=soft-timeout
# volname=glustervol
# client_hosts=10.70.46.23,10.70.46.24
# time=100
#
#
#
# For setting hard-timeout
#
# [quota]
# action=hard-timeout
# volname=glustervol
# client_hosts=10.70.46.23,10.70.46.24
# time=100
